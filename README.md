# ðŸ¦ Bank Customer Churn Prediction â€” End-to-End ML Solution

> **A complete, production-ready machine learning project that predicts customer churn for a bank.** Built with modern MLOps practices, professional tooling, and a focus on deployment and explainability.

---

## ðŸ“– Overview

I built this project as a **complete end-to-end machine learning solution** to tackle one of the most critical business problems: **predicting which bank customers are likely to leave**. This isn't just a notebook â€” it's a **real-world production system** that showcases how to take a machine learning idea from data exploration all the way to a deployed, interactive web application.

### What Does It Do?

The system **analyzes customer banking data** (credit score, account balance, tenure, activity, etc.) and **predicts with high accuracy whether a customer will churn**. Banks can use this to:
- ðŸŽ¯ **Identify at-risk customers** before they leave
- ðŸ’¼ **Prioritize retention campaigns** more effectively
- ðŸ“Š **Make data-driven business decisions**

---

## ðŸ› ï¸ Tech Stack & Architecture

This project demonstrates **professional MLOps practices** across the entire ML lifecycle:

### ðŸ”¬ **Data & ML Stack**
| Layer | Tools | Purpose |
|-------|-------|---------|
| **Data Processing** | ðŸ¼ `pandas`, `scikit-learn` | Load, clean, and transform raw customer data |
| **Feature Engineering** | ðŸ”§ Custom pipelines in `src/features/` | Create meaningful features from raw data |
| **Model Training** | ðŸŽ¯ `XGBoost` (+ Optuna tuning) | Gradient-boosted classification model with hyperparameter optimization |
| **Experiment Tracking** | ðŸ“Š **MLflow** | Log metrics, parameters, models, and artifacts for every run |
| **Serving & API** | âš¡ **FastAPI** | High-performance REST API endpoint (`/predict`) |
| **User Interface** | ðŸŽ¨ **Gradio** | Interactive web UI for real-time predictions (`/ui`) |

### ðŸ“¦ **MLOps & Deployment**
| Aspect | Tools | Details |
|--------|-------|---------|
| **Model Artifacts** | ðŸ“ `joblib` + XGBoost JSON | Serialized model and preprocessor saved for reproducibility |
| **Hyperparameter Tuning** | ðŸ” **Optuna** | Automated bayesian optimization searching 30+ trial configurations |
| **Preprocessing Pipeline** | ðŸ”„ Sklearn transformers | Reusable, versioned preprocessing stored with model |
| **Experiment Logging** | ðŸ“ˆ **MLflow** (local storage in `mlruns/`) | Every training run logged with metrics (accuracy, recall) and model artifacts |
| **Containerization** | ðŸ³ `Dockerfile` | Application packaged for cloud/Docker deployment |

### ðŸš€ **Deployment Ready**
- âœ… **REST API** for programmatic predictions
- âœ… **Web UI** for interactive use
- âœ… **Health check** endpoint (`GET /`)
- âœ… **Input validation** with Pydantic models
- âœ… **Error handling** and fallback mechanisms

---

## ðŸ“ Project Structure (Organized for Scale)

```
.
â”œâ”€â”€ ðŸ“‚ src/                          # Source code (main application)
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ app.py                   # FastAPI app + Gradio UI integration
â”‚   â”‚   â””â”€â”€ main.py                  # Entry point
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ load_data.py             # CSV loading logic
â”‚   â”‚   â””â”€â”€ preprocess.py            # Data cleaning & normalization
â”‚   â”œâ”€â”€ features/
â”‚   â”‚   â””â”€â”€ build_features.py        # Feature engineering (one-hot, scaling, etc.)
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ train.py                 # Model training with MLflow logging
â”‚   â”‚   â”œâ”€â”€ tune.py                  # Hyperparameter tuning (Optuna)
â”‚   â”‚   â””â”€â”€ evaluate.py              # Performance metrics & reporting
â”‚   â””â”€â”€ serving/
â”‚       â””â”€â”€ inference.py             # Prediction logic (model loading, preprocessing)
â”‚
â”œâ”€â”€ ðŸ“‚ scripts/
â”‚   â””â”€â”€ pipeline.py                  # End-to-end training orchestration
â”‚
â”œâ”€â”€ ðŸ“‚ notebooks/
â”‚   â”œâ”€â”€ EDA.ipynb                    # Exploratory data analysis & model experimentation
â”‚   â””â”€â”€ models/
â”‚       â””â”€â”€ final_xgboost_model.json # Exported model from notebook
â”‚
â”œâ”€â”€ ðŸ“‚ data/
â”‚   â””â”€â”€ train.csv                    # Raw customer dataset
â”‚
â”œâ”€â”€ ðŸ“‚ models/                       # Serialized artifacts (generated by pipeline)
â”‚   â”œâ”€â”€ model.joblib                 # Trained XGBoost classifier
â”‚   â””â”€â”€ preprocessor.joblib          # Fitted preprocessing pipeline
â”‚
â”œâ”€â”€ ðŸ“‚ mlruns/                       # MLflow experiment tracking (local)
â”‚   â””â”€â”€ [tracked runs with metrics, params, artifacts]
â”‚
â”œâ”€â”€ Dockerfile                       # Container image for deployment
â”œâ”€â”€ requirements.txt                 # Python dependencies
â””â”€â”€ README.md                        # This file
```

---

## ðŸš€ Quick Start (5 Minutes)

### 1ï¸âƒ£ **Setup Python Environment**
```bash
# Create virtual environment
python -m venv .venv

# Activate it
source .venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### 2ï¸âƒ£ **Train the Model** (regenerate artifacts)
```bash
# Run the complete pipeline:
# Load data â†’ Preprocess â†’ Feature engineering â†’ Train â†’ Evaluate
.venv/bin/python scripts/pipeline.py
```

**What happens:**
- ðŸ“¥ Loads customer data
- ðŸ§¹ Cleans & preprocesses features
- âš™ï¸ Builds engineered features
- ðŸŽ¯ Trains XGBoost with pre-tuned hyperparameters
- ðŸ“Š Evaluates on holdout test set
- ðŸ’¾ Saves `models/model.joblib` and `models/preprocessor.joblib`
- ðŸ“ˆ Logs metrics to MLflow (`mlruns/`)

### 3ï¸âƒ£ **Launch the Web App**
```bash
# Start FastAPI + Gradio UI
python -m src.app.main

# Opens at http://localhost:8000/ui
```

**What you get:**
- ðŸŽ¨ **Interactive form** to enter customer details
- âš¡ **Real-time predictions** with churn probability
- ðŸ”Œ **REST API** at `http://localhost:8000/predict` (POST)

### 4ï¸âƒ£ **Make a Prediction (API)**
```bash
curl -X POST http://localhost:8000/predict \
  -H 'Content-Type: application/json' \
  -d '{
    "CreditScore": 650,
    "Geography": "France",
    "Gender": "Male",
    "Age": 35,
    "Tenure": 5,
    "Balance": 50000,
    "NumOfProducts": 1,
    "HasCrCard": 1,
    "IsActiveMember": 1,
    "EstimatedSalary": 50000
  }'
```

**Response:**
```json
{
  "label": "Not Churn",
  "churn": false,
  "probability": 0.42
}
```

---

## ðŸ”¬ What Each Component Does

### ðŸ“Š **`src/data/` â€” Data Loading & Preprocessing**
- Loads CSV data with `pandas`
- Handles missing values, scales numeric features
- Encodes categorical variables (Geography, Gender)
- **Reproducible pipeline** â€” same preprocessing for training and serving

### âš™ï¸ **`src/features/` â€” Feature Engineering**
- One-hot encoding of categorical columns
- Outlier detection & handling
- Feature scaling (StandardScaler)
- Creates new derived features for better model performance

### ðŸŽ¯ **`src/models/` â€” ML Training Logic**

#### **`train.py`** â€” Main Training Module
```python
# Key features:
âœ… XGBoost classifier with tuned hyperparameters
âœ… Handles class imbalance (weighted loss)
âœ… 80/20 train-test split with stratification
âœ… MLflow integration: logs metrics, parameters, model artifacts
âœ… Threshold-based predictions (prob > 0.5 for churn)
```

#### **`tune.py`** â€” Hyperparameter Optimization (Optuna)
```python
# Bayesian optimization over:
âœ… n_estimators (300â€“800 boosting rounds)
âœ… learning_rate, max_depth, subsample ratios
âœ… Regularization: gamma, alpha, lambda
âœ… Objective: maximize recall for churners (catch at-risk customers)
âœ… Constraint: maintain >75% accuracy
âœ… 5-fold cross-validation for robustness
```

#### **`evaluate.py`** â€” Model Evaluation
- Computes accuracy, precision, recall, F1-score
- Generates classification reports
- Produces confusion matrices & ROC curves

### ðŸ”Œ **`src/serving/inference.py`** â€” Prediction Engine
**Production-ready inference with:**
- âœ… **Smart artifact loading** â€” tries multiple paths (legacy + repo-level)
- âœ… **Fallback preprocessing** â€” if pickled preprocessor fails, uses code-based pipeline
- âœ… **Feature alignment** â€” dynamically aligns input features to model's expected schema
- âœ… **Type coercion** â€” ensures numeric inputs for XGBoost
- âœ… **Probability thresholding** â€” converts soft probabilities to binary churn/no-churn

### âš¡ **`src/app/` â€” API & UI**
- **FastAPI app** (`app.py`) with `/predict` endpoint
- **Pydantic data validation** for input safety
- **Gradio interface** mounted at `/ui` for non-technical users
- **Error handling** â€” graceful fallbacks on missing features or bad input

---

## ðŸ“ˆ MLOps & Experiment Tracking (MLflow)

Every model training run is logged to **MLflow** for reproducibility and comparison:

### View Experiment Results
```bash
# Launch MLflow UI (browse all runs, compare metrics, download artifacts)
mlflow ui --backend-store-uri mlruns

# Opens at http://localhost:5000
```

### What Gets Logged
- ðŸ“Š **Metrics:** accuracy, recall, precision, F1-score
- ðŸ”§ **Parameters:** hyperparameters (n_estimators, learning_rate, etc.)
- ðŸ’¾ **Artifacts:** serialized model (joblib), preprocessor, evaluation plots
- ðŸ“ **Runs:** timestamp, duration, status

This makes it **easy to compare runs, track improvements, and debug**:
```
Run 1: accuracy=0.748, recall=0.870  â† Best model
Run 2: accuracy=0.742, recall=0.860
Run 3: accuracy=0.735, recall=0.845
```

---

## ðŸ³ Deployment (Docker & CI/CD Ready)

### Build Docker Image
```bash
docker build -t bank-churn:latest .
```

### Run Container
```bash
docker run -p 8000:8000 bank-churn:latest
# Access UI at http://localhost:8000/ui
```

### Why Docker?
âœ… Reproducible environment across machines
âœ… Easy deployment to cloud (AWS, GCP, Azure)
âœ… Isolated dependencies (no conflicts)
âœ… Production-grade isolation

### CI/CD Integration (Recommended)
You can automate:
- âœ… **Testing:** run pytest on new commits
- âœ… **Training:** retrain model on schedule or new data
- âœ… **Artifact management:** upload models to registries
- âœ… **Deployment:** push to staging/production automatically

Example GitHub Actions workflow (not included, but easy to add):
```yaml
on: [push]
jobs:
  test-and-train:
    runs-on: ubuntu-latest
    steps:
      - run: pip install -r requirements.txt
      - run: pytest src/
      - run: python scripts/pipeline.py
      - run: docker build -t churn-model .
```

---

## ðŸ”‘ Key Technical Decisions & Highlights

### âœ¨ **Why XGBoost?**
- Handles tabular data **exceptionally well**
- Fast training & inference
- Built-in feature importance insights
- Robust to outliers & scaling issues

### âœ¨ **Why Optuna for Tuning?**
- **Bayesian optimization** (smarter than grid search)
- Prunes unpromising trials early
- Found optimal hyperparameters in 30 trials (vs. 1000+ grid search)

### âœ¨ **Why MLflow?**
- **Central experiment hub** â€” no scattered notebooks
- Easy to compare runs & metrics
- Reproducible artifacts (retrain any historical model)
- Industry standard (used at Netflix, Uber, etc.)

### âœ¨ **Why FastAPI + Gradio?**
- **FastAPI:** lightning-fast REST API, automatic OpenAPI docs
- **Gradio:** beautiful UI from Python function (no JavaScript needed)
- **Together:** production API + user-friendly interface in one app

### âœ¨ **Robustness Features**
- **Fallback preprocessing:** if pickled preprocessor fails, use code-based pipeline
- **Feature alignment:** automatically fills missing columns, reorders to model spec
- **Type safety:** Pydantic validates all inputs before inference

---

## ðŸ“Š Model Performance

After hyperparameter tuning (via Optuna) and training:

| Metric | Score |
|--------|-------|
| **Accuracy** | ~0.75 (correctly identifies 75% of customers) |
| **Recall** | ~0.87 (catches 87% of actual churners) |
| **Precision** | ~0.60 (when predicting churn, 60% are true positives) |
| **F1-Score** | ~0.71 |

**Business Insight:** High recall means we rarely miss an at-risk customer (good for retention campaigns), though some false alarms (lower precision) are acceptable in banking.

---

## ðŸ§  What I Learned & Demonstrated

This project showcases **production ML best practices**:

### âœ… **Data Engineering**
- Structured data pipelines (load â†’ preprocess â†’ feature engineer)
- Handling class imbalance (scale_pos_weight in XGBoost)
- Reproducible preprocessing (same code for training & serving)

### âœ… **Model Engineering**
- Hyperparameter tuning (Optuna bayesian search)
- Cross-validation for robust evaluation
- Threshold tuning (not just argmax probability)
- Experiment tracking (MLflow)

### âœ… **Software Engineering**
- Modular code structure (`src/` layout)
- Separation of concerns (data, features, models, serving)
- Type hints & validation (Pydantic)
- Error handling & fallbacks
- Containerization (Docker)

### âœ… **MLOps & Deployment**
- REST API design
- Model versioning & artifact management
- Interactive UI (Gradio)
- Monitoring-ready architecture
- Local experiment tracking (MLflow)

---

## ðŸš€ Next Steps (Production Enhancements)

To take this further in a real company:

1. **Model Registry:** Use MLflow Model Registry or cloud alternatives (AWS SageMaker, GCP Vertex AI)
2. **Data Pipeline:** Add scheduled retraining (Airflow, Prefect, or cloud-native DAGs)
3. **Monitoring:** Track model drift & prediction distribution in production
4. **A/B Testing:** Compare new models against incumbent in shadow traffic
5. **Feature Store:** Centralize feature definitions (Feast, Tecton)
6. **API Authentication:** Add API keys, rate limiting, audit logging
7. **Unit Tests:** Pytest for inference logic, data validation
8. **Documentation:** API docs (auto-generated by FastAPI), runbooks

---

## ðŸ“š How to Explore Further

### ðŸ”¬ **Dive into the Notebook**
```bash
# Open EDA.ipynb in Jupyter to see:
# - Data exploration & visualizations
# - Model experimentation & comparison
# - Manual threshold tuning
jupyter notebook notebooks/EDA.ipynb
```

### ðŸ“ˆ **Check Experiment Results**
```bash
mlflow ui --backend-store-uri mlruns
# Browse all training runs, metrics, and saved models
```

### ðŸ§ª **Run Tests & Debug**
```bash
# Import the inference function and test locally
python -c "from src.serving.inference import predict; print(predict({...}))"
```

---

## ðŸ“‹ Requirements & Installation

**Python 3.10+** (tested on 3.10, 3.11)

```
pandas>=1.3.0
scikit-learn>=1.0.0
xgboost>=1.5.0
optuna>=2.10.0        # Hyperparameter tuning
mlflow>=1.20.0        # Experiment tracking
fastapi>=0.95.0       # REST API
uvicorn>=0.20.0       # ASGI server
gradio>=3.0.0         # Web UI
joblib>=1.0.0         # Model serialization
```

---

## ðŸŽ¯ Summary

This project is a **complete, production-grade machine learning solution** that demonstrates:
- âœ… End-to-end ML pipeline (data â†’ features â†’ model â†’ serving)
- âœ… Modern MLOps tooling (MLflow, Optuna, XGBoost)
- âœ… Professional code structure & practices
- âœ… Deployed API + interactive UI
- âœ… Robustness & fallback mechanisms
- âœ… Experiment tracking & reproducibility
- âœ… Docker containerization

It's not just a proof-of-concept â€” it's **ready for a small-to-medium scale production deployment** and demonstrates the thinking of a **professional ML engineer**.

---

